{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b9f7ed",
   "metadata": {},
   "source": [
    "# Uczenie maszynowe w Python - Zaliczenie\n",
    ">Authors: \n",
    "><ul>\n",
    ">    <li>Michal Gasiorek</li>\n",
    ">    <li>Piotr Bosianek</li>\n",
    ">    <li>Tomasz Kurek</li>\n",
    "></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf91c9",
   "metadata": {},
   "source": [
    "## Predicting spam e-mails\n",
    "This notebook looks into Python-based machine learning and data-science libraries to build a machine learning model for detecting spam e-mails\n",
    "\n",
    "## Content\n",
    "<ol>\n",
    "    <li>Problem Definition</li>\n",
    "    <li>Data</li>\n",
    "    <li>Features</li>\n",
    "    <li>Preparing the tools</li>\n",
    "    <li>Classes</li>\n",
    "        <ul>\n",
    "            <li>Data Preparator</li>\n",
    "            <li>Data Cleaner</li>\n",
    "            <li>Plots</li>\n",
    "            <li>Vector And Train</li>\n",
    "            <li>Model Evaluator</li>\n",
    "        </ul>\n",
    "    <li>Data analysis</li>\n",
    "    <li>Data Cleaning</li>\n",
    "    <li>Data Vectorizing</li>\n",
    "    <li>Traning models</li>\n",
    "    <li>Scoring models</li>\n",
    "    <li>Testing models</li>\n",
    "</ol>\n",
    "\n",
    "### 1. Problem Definition\n",
    "> Given e-mail message classify it as spam message or ham (not spam)\n",
    "\n",
    "### 2. Data\n",
    "Data was downloaded from: https://www.kaggle.com/venky73/spam-mails-dataset\n",
    "\n",
    "### 3. Features\n",
    "<ol>\n",
    "    <li>label - labels of e-mails which can be either Spam or Ham</li>\n",
    "    <li>text - e-mail subject and message</li>\n",
    "    <li>label_num: \n",
    "        <ul>\n",
    "            <li>0 - ham</li>\n",
    "            <li>1 - spam</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47223a",
   "metadata": {},
   "source": [
    "### 4. Preparing the tools\n",
    "We will use NumPy, Pandas, Matplotlib for data analysis and manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "328badd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\PC\n",
      "[nltk_data]     COMPUTER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\PC\n",
      "[nltk_data]     COMPUTER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# All import we will need:\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from textblob import Word\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import unittest\n",
    "TestCase.maxDiff = None\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeddca4",
   "metadata": {},
   "source": [
    "## 5. Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a1cb1e",
   "metadata": {},
   "source": [
    "**Data Preparator** \n",
    "> handles preparing and analyzing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e781aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparator:\n",
    "    \n",
    "    def __init__(self, file_name):\n",
    "        self.df = pd.read_csv(file_name)\n",
    "        self.email_amount = 0\n",
    "        self.ham_amount = 0\n",
    "        self.spam_amount = 0\n",
    "        self.word_count = 0\n",
    "        self.avg_words = 0       \n",
    "    \n",
    "    # print out the information about the dataset\n",
    "    def __str__(self):\n",
    "        data_info = f\"\"\" Dataset contains **{self.email_amount}** emails<br> \n",
    "                    Messages marked as HAM: **{self.ham_amount}**<br>\n",
    "                    Messages marked as SPAM: **{self.spam_amount}**<br>\n",
    "                    Total words count: **{self.word_count}** words<br>\n",
    "                    Average words per e-mail:  **~{self.avg_words}**\"\"\"\n",
    "                \n",
    "        display(Markdown(data_info))\n",
    "        return ''\n",
    "    \n",
    "    def get_dataframe_head(self, amount):\n",
    "        return self.df.head(amount)\n",
    "    \n",
    "    def analize(self):\n",
    "        # label the unnamed column with ids\n",
    "        self.df.rename(columns={'Unnamed: 0': 'id'}, inplace = True)\n",
    "\n",
    "        # amount of emails in the dataset\n",
    "        self.email_amount = self.df.shape[0]\n",
    "        \n",
    "        \n",
    "        # Series with the amount of words in each email and their total count\n",
    "        words_in_emails = (self.df['text'].apply(lambda x: len(str(x).split(\" \"))))\n",
    "        self.word_count = np.sum(words_in_emails)\n",
    "        \n",
    "        # average amount of words in each email\n",
    "        self.avg_words = round(np.divide(self.word_count, self.email_amount), 2)\n",
    "        \n",
    "        # amount of spam and ham emails \n",
    "        ham_and_spam_amount= self.df.groupby(['label']).count()\n",
    "        self.ham_amount = ham_and_spam_amount.iloc[0, 0]\n",
    "        self.spam_amount = ham_and_spam_amount.iloc[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768fc6e",
   "metadata": {},
   "source": [
    "**Data Cleaner** \n",
    "> handles cleaning the data (removes stopwords and lemmatize e-mail messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a98242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    \n",
    "    def __init__(self, analyzed_data):\n",
    "        self.df = analyzed_data.df\n",
    "        \n",
    "    \n",
    "    def applying_clean_contents(self):\n",
    "        print(\"Cleaning contents...\")\n",
    "        # apply the function to each row of the dataframe\n",
    "        self.df['text_clean'] = self.df['text'].apply(lambda string: clean_contents(string))\n",
    "        print(\"Contents cleaned!\")\n",
    "        return self.df.head() # printing 5 rows after\n",
    "    \n",
    "       \n",
    "    def cleaning_stopwords(self):\n",
    "        print('Cleaning stop words...')\n",
    "        stop = stopwords.words('english')\n",
    "        self.df['text_clean'] = self.df['text_clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "        print('Stop words removed!')\n",
    "        return self.df.head()\n",
    "        \n",
    "    # lemmatize the cleaned email contents\n",
    "    def lemmatization(self):\n",
    "        print('Run lemmatization...')\n",
    "        self.df['text_clean'] = self.df['text_clean'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "        print('Lemmatization complete!')\n",
    "        return self.df.head(10)\n",
    "        \n",
    "    def clean_most_frequent_words(self):\n",
    "        print(\"Removing most frequent words...\")\n",
    "        # define the most frequent words in the emails\n",
    "        frequent_words = pd.Series(' '.join(self.df['text_clean']).split()).value_counts()\n",
    "        frequent_words = frequent_words[frequent_words > 1000]\n",
    "        \n",
    "        # remove them from the email contents\n",
    "        frequent_words = list(frequent_words.index)\n",
    "        self.df['text_clean'] = self.df['text_clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in frequent_words))\n",
    "        print(\"Most frequent words removed!\")\n",
    "        return self.df.head(10)\n",
    "        \n",
    "# clean the contents of each email\n",
    "def clean_contents(string, reg = RegexpTokenizer(r'[a-z]+')):\n",
    "    string = string.lower()\n",
    "    tokens = reg.tokenize(string)\n",
    "    return \" \".join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77367d96",
   "metadata": {},
   "source": [
    "**Plots** \n",
    "> handles plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0344de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plots:\n",
    "    \n",
    "    def __init__(self, cleaned_data):\n",
    "        self.df = cleaned_data.df\n",
    "        \n",
    "    def plot_ham_to_spam_ratio(self):\n",
    "        self.df['label'].value_counts().plot.bar(color = [\"g\",\"r\"])\n",
    "        plt.title('Total number of ham and spam in the dataset')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_accuracy(self, models):\n",
    "        # show models accuracy as bar chart:\n",
    "        model_names = models['Model']\n",
    "        model_acc = models['Accuracy']\n",
    "\n",
    "        plt.xticks(np.arange(len(model_names)), model_names)\n",
    "        plt.bar(np.arange(len(model_names)), model_acc*100, color=['r', 'g', 'b', 'y'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e869335",
   "metadata": {},
   "source": [
    "**Vector and Train** \n",
    "> creates vectors and splits the dataset to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a857ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorAndTrain:\n",
    "    def __init__(self, cleaned_data):\n",
    "        self.df = cleaned_data.df\n",
    "        self.cv = CountVectorizer()\n",
    "    \n",
    "    def get_cv(self):\n",
    "        return self.cv\n",
    "    def vectorization(self):\n",
    "        # Convert a collection of text documents to a matrix of token counts\n",
    "        X = self.cv.fit_transform(self.df.text_clean)\n",
    "        # Get the categories\n",
    "        y = self.df.label\n",
    "        # Words exctracted from text_clean\n",
    "        print(self.cv.get_feature_names()[10:50:2])\n",
    "        return X, y\n",
    "    \n",
    "    def train(self, X ,y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff498c3",
   "metadata": {},
   "source": [
    "**Model Evaluator** \n",
    "> handles training and scoring given models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0292fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ModelEvaluator():\n",
    "    #constructor\n",
    "    def __init__(self, cv):\n",
    "        self.models = []\n",
    "        self.cv = cv\n",
    "        \n",
    "    def add(self, modelName, model):\n",
    "        self.models.append({'name': modelName, 'model': model, 'acc': 0, 'time_trained': 0 })\n",
    "    \n",
    "    # returns models as dataframe\n",
    "    def get_models(self):    \n",
    "        models_arr = []\n",
    "        \n",
    "        for item in self.models:\n",
    "            name, model, acc, time_trained = item.values()\n",
    "            models_arr.append([name, acc, time_trained])\n",
    "        \n",
    "        models_df = pd.DataFrame(models_arr) # create dataframe out of the models array\n",
    "        models_df.columns = ['Model', 'Accuracy', 'Time Trained'] # add columns \n",
    "        \n",
    "        # sort values by model's accuracy\n",
    "        models_df.sort_values(by='Accuracy', ascending = False, inplace=True) \n",
    "        \n",
    "        #reset index\n",
    "        models_df.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        return models_df\n",
    "        \n",
    "    # handels traing given models\n",
    "    def train(self, X_train, y_train):\n",
    "        print(\"Run Training...\")\n",
    "        for item in self.models:\n",
    "            #desctuct values from each item\n",
    "            name, model, acc, time_trained = item.values()\n",
    "            \n",
    "            # calculate the time and train the model\n",
    "            start = time.time()\n",
    "            print(f\"Training {name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            end = time.time()\n",
    "            time_trained = round(end - start,2)\n",
    "            print(f'{model} trained in {time_trained} seconds')\n",
    "            \n",
    "            #save trained model and time\n",
    "            item['model'] = model\n",
    "            item['time_trained'] = time_trained\n",
    "        print(\"Training completed successfully!\")\n",
    "        \n",
    "    # handels scoring given models\n",
    "    def score(self, X_test, y_test):\n",
    "        print(\"Run Scoring...\")\n",
    "        for item in self.models:\n",
    "            # desctruct values of each item\n",
    "            name, model, acc, time_trained = item.values()\n",
    "            \n",
    "            # calculate the accuracy\n",
    "            acc = model.score(X_test, y_test)\n",
    "            display(Markdown(f'{name} scored with {round(acc*100, 2)}%'))\n",
    "            # save accuracy: \n",
    "            item['acc'] = acc\n",
    "        print(\"Scoring completed successfully!\")\n",
    "    \n",
    "    # checks which model did best and returs it\n",
    "    def get_most_accurate_model(self):\n",
    "        best_model = None\n",
    "        max_score = 0\n",
    "        \n",
    "        for item in self.models:\n",
    "            # destruct values of each item:\n",
    "            name, model, acc, time_trained = item.values()\n",
    "            \n",
    "            # check if accuracy is best\n",
    "            if(acc > max_score):\n",
    "                max_score = acc\n",
    "                best_model = model\n",
    "\n",
    "        # return best model\n",
    "        return best_model\n",
    "    \n",
    "    # handels testing model\n",
    "    def test(self, model,  text_message, label):\n",
    "        display(Markdown((f\"Model used **{model}**\" )))\n",
    "        message_vec = self.cv.transform([text_message])\n",
    "        prediction = model.predict(message_vec)\n",
    "        \n",
    "        display(Markdown((f\"#### Given Label: **{label}**\")))\n",
    "        display(Markdown((f\"#### Predicted label: **{prediction[0]}**\" )))\n",
    "        display(Markdown((f\"**Email message:**\")))\n",
    "        display(Markdown((text_message)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d7ce40",
   "metadata": {},
   "source": [
    "### 6. Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950330d4",
   "metadata": {},
   "source": [
    "#### Preparate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bca5fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get data from preparator:\n",
    "data = DataPreparator(\"spam_ham_dataset.csv\")\n",
    "\n",
    "# analize the data: \n",
    "data.analize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5180ee",
   "metadata": {},
   "source": [
    "#### Show 10 rows of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9c1b51f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2949</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: ehronline web address change\\r\\nthis ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2793</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: spring savings certificate - take 30 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4185</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: looking for medication ? we ` re the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2641</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: noms / actual flow for 2 / 26\\r\\nwe a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1870</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: nominations for oct . 21 - 23 , 2000\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id label                                               text  label_num\n",
       "0   605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...          0\n",
       "1  2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...          0\n",
       "2  3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...          0\n",
       "3  4685  spam  Subject: photoshop , windows , office . cheap ...          1\n",
       "4  2030   ham  Subject: re : indian springs\\r\\nthis deal is t...          0\n",
       "5  2949   ham  Subject: ehronline web address change\\r\\nthis ...          0\n",
       "6  2793   ham  Subject: spring savings certificate - take 30 ...          0\n",
       "7  4185  spam  Subject: looking for medication ? we ` re the ...          1\n",
       "8  2641   ham  Subject: noms / actual flow for 2 / 26\\r\\nwe a...          0\n",
       "9  1870   ham  Subject: nominations for oct . 21 - 23 , 2000\\...          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 10 rows from dataset: \n",
    "data.get_dataframe_head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94b22be",
   "metadata": {},
   "source": [
    "#### Show summary of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6752aa36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Dataset contains **5171** emails<br> \n",
       "                    Messages marked as HAM: **3672**<br>\n",
       "                    Messages marked as SPAM: **1499**<br>\n",
       "                    Total words count: **1083244** words<br>\n",
       "                    Average words per e-mail:  **~209.48**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# print informations about dataset:\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed8aaa3",
   "metadata": {},
   "source": [
    "#### Plot the amount of ham and spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20498177",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = Plots(data)\n",
    "plot.plot_ham_to_spam_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c057d651",
   "metadata": {},
   "source": [
    "### 7. Data Cleaning\n",
    "> We are removing stop words and most frequent words\n",
    "> Also on we run lemmatization on the e-mail messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68522b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning contents...\n",
      "Contents cleaned!\n",
      "Cleaning stop words...\n",
      "Stop words removed!\n",
      "Run lemmatization...\n",
      "Lemmatization complete!\n",
      "Removing most frequent words...\n",
      "Most frequent words removed!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Message before cleaning:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Subject: enron methanol ; meter # : 988291\r\n",
       "this is a follow up to the note i gave you on monday , 4 / 3 / 00 { preliminary\r\n",
       "flow data provided by daren } .\r\n",
       "please override pop ' s daily volume { presently zero } to reflect daily\r\n",
       "activity you can obtain from gas control .\r\n",
       "this change is needed asap for economics purposes ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Message after cleaning:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "methanol follow note gave monday preliminary flow data provided override pop daily presently zero reflect daily activity obtain control needed asap economics purpose"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean Data\n",
    "clean_data = DataCleaner(data)\n",
    "\n",
    "# Run cleaning\n",
    "clean_data.applying_clean_contents()\n",
    "clean_data.cleaning_stopwords()\n",
    "clean_data.lemmatization()\n",
    "data_head = clean_data.clean_most_frequent_words()\n",
    "\n",
    "display(Markdown(\"**Message before cleaning:**\"))\n",
    "display(Markdown(data_head['text'][0]))\n",
    "\n",
    "display(Markdown(\"**Message after cleaning:**\"))\n",
    "display(Markdown(data_head['text_clean'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd3eaa",
   "metadata": {},
   "source": [
    "### 8. Data Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d5e9aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaihmqv', 'aalland', 'aamlrg', 'aare', 'aaron', 'aavilable', 'ab', 'ababa', 'aback', 'abacus', 'abandon', 'abandoned', 'abasement', 'abashed', 'abater', 'abb', 'abbasi', 'abbey', 'abbott', 'abbreviation']\n"
     ]
    }
   ],
   "source": [
    "# Create vectors form messages \n",
    "vectors = VectorAndTrain(clean_data)  \n",
    "X, y = vectors.vectorization()\n",
    "\n",
    "# split dataset to train and test:\n",
    "X_train, X_test, y_train, y_test = vectors.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb5a1b",
   "metadata": {},
   "source": [
    "### 9. Model Training\n",
    "> We are choosing most common Classifiers to train on our cleaned dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28f0881d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Training...\n",
      "Training RandomForest...\n",
      "RandomForestClassifier() trained in 5.82 seconds\n",
      "Training KNeighbors...\n",
      "KNeighborsClassifier() trained in 0.0 seconds\n",
      "Training LogisticRegress...\n",
      "LogisticRegression() trained in 0.26 seconds\n",
      "Training DecisionTree...\n",
      "DecisionTreeClassifier() trained in 1.05 seconds\n",
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create model evaluator\n",
    "modelEval = ModelEvaluator(vectors.get_cv())\n",
    "\n",
    "# add couple of models to model evaluator \n",
    "modelEval.add(\"RandomForest\", RandomForestClassifier())\n",
    "modelEval.add(\"KNeighbors\", KNeighborsClassifier())\n",
    "modelEval.add(\"LogisticRegress\", LogisticRegression())\n",
    "modelEval.add(\"DecisionTree\", DecisionTreeClassifier())\n",
    "\n",
    "# train models\n",
    "modelEval.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57a3e8",
   "metadata": {},
   "source": [
    "### 10. Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e458db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Scoring...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "RandomForest scored with 97.1%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "KNeighbors scored with 78.45%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "LogisticRegress scored with 97.39%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "DecisionTree scored with 92.46%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring completed successfully!\n"
     ]
    }
   ],
   "source": [
    "#score the models\n",
    "modelEval.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8063333a",
   "metadata": {},
   "source": [
    "#### Checking which model did best\n",
    "> It seems that Logistic Reggresion and Random Forest did best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b16827de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get all trained models\n",
    "models_df = modelEval.get_models()\n",
    "plot.plot_accuracy(models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d33d6f",
   "metadata": {},
   "source": [
    "### 11. Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44510b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Model used **LogisticRegression()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Given Label: **spam**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Predicted label: **spam**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Email message:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "photoshop window office cheap main trending abasement darer prudently fortuitous undergone lighthearted charm orinoco taster railroad affluent pornographic cuvier irvin parkhouse blameworthy chlorophyll robed diagrammatic fogarty clear bayda inconveniencing managing represented smartness hashish academy shareholder unload badness danielson pure caffein spaniard chargeable levin"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Model used **LogisticRegression()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Given Label: **spam**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Predicted label: **spam**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Email message:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "looking medication best source difficult make material condition better best law easy enough ruin bad law excuse found best simpliest site medication net perscription easy delivery private secure easy better rightly pound week squint million got anything ever want erection treatment pill anti depressant pill weight loss splicing bombahakcx knowledge human power synonymous high quality stuff low rate moneyback guarantee god nature sufficeth unto wise hath author"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Model used **LogisticRegression()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Given Label: **spam**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Predicted label: **spam**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Email message:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "vocable rnd word asceticism vcsc brand stock attention vocalscape inc stock symbol vcsc vcsc top stock pick april stock expected bounce cent level stock hit low bounce back stock going explode next watch soar watch stock go crazy next week breaking news vocalscape inc announces agreement resell mix network service current expect projected speculative next expect projected speculative next vocalscape network inc building revolutionizing telecommunication industry affordable phone system hardware online software rate canada vocalscape global reach receiving international attention development voice ip voip application solution including award winning eyefontm softphone real pc phone advanced implementer pbx system call center itsps service provider vocalscape created software interactive solution revolving around global communication data voice convergence use vocalscape voice internet protocol application like ip pbxs softswitches pc phone web phone providing real human interaction delivery internet vocalscape solution business offer quality voice service anywhere world rate significantly lower current long distance charge develop software run voip network sell install service branded voip gateway gatekeeper control software also license software customer want brand voip solution vocalscape committed making great technology challenging status quo building st century way business communicate interact internet current expect projected speculative next expect projected speculative next breaking news vocalscape inc announces agreement resell mix network service katonah n prnewswire firstcall via comtex vocalscape inc pink vcsc emerging leader development voice internet protocol voip telephony solution announced today entered sale agent agreement mix network inc voip enhanced telephony service provider agreement provides vocalscape turnkey calling card customer access mix network service including north american dids phone number domestic long distance termination vocalscape also able supply client enhanced voip product including pre paid calling mix network north american network excited help launch voip business model using solution whole picture software network needed route call mix network give ability offer wholesale monthly flat rate plan dids client allow build business model like popular voip vonage packet say ryan gibson vp vocalscape network legal word continue within email contains forward looking statement within meaning section security act section b security exchange act statement express involve discussion respect prediction goal expectation belief plan projection objective assumption future event performance statement historical fact forward looking statement forward looking statement based expectation estimate projection statement made involve number risk uncertainty could cause actual result event differ materially presently anticipated forward looking statement action identified use word project foresee expects estimate belief understands part anticipates statement indicating certain action could might occur provided within email pertaining investing stock security must understood provided investment advice emerging equity alert advises reader subscriber seek advice registered professional security representative deciding trade stock featured within email none material within report shall construed kind investment advice mind interpretation witer newsletter news published represent official statement fact differ real meaning news release meant say look news release judge detail compliance section b disclose holding vcsc share prior publication report aware inherent conflict interest resulting holding due intent profit liquidation share share sold even positive statement made regarding since share inherent conflict interest statement opinion reader publication cautioned place undue reliance forward looking statement based certain assumption expectation involving various risk uncertainty could cause result differ materially set forth forward looking statement advised nothing within email shall constitute solicitation invitation position sell security mentioned herein newsletter neither registered investment advisor affiliated broker dealer newsletter paid third party send report statement made express opinion treated take position sell security mentioned report includes forward looking statement within meaning private security litigation reform act statement include term projected speculative expect believe soar move undervalued intend similar term"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Model used **LogisticRegression()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Given Label: **ham**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Predicted label: **ham**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Email message:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "methanol follow note gave monday preliminary flow data provided override pop daily presently zero reflect daily activity obtain control needed asap economics purpose"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Model used **LogisticRegression()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Given Label: **ham**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Predicted label: **ham**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Email message:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "nom january file hplnol hplnol"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Model used **LogisticRegression()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Given Label: **ham**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Predicted label: **ham**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Email message:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "neon retreat ho ho ho around wonderful year neon leader retreat year extremely hectic tough think anything past holiday life go past week december january like think minute calender handed beginning fall semester retreat scheduled weekend january youth minister conference brad dustin connected week going date following weekend january come part think think agree important together recharge battery far spring semester lot trouble difficult away without kid etc brad came potential alternative together weekend prefer first option retreat similar done past several year year could go heartland country inn www outside brenham nice place bedroom bedroom house side side country real relaxing also close brenham one hour minute golf shop antique craft store brenham eat dinner together ranch spend meet saturday return sunday morning like done past second option stay houston dinner together nice restaurant dessert visiting recharging one home saturday evening might easier trade much together decide email back preference course available weekend democratic process prevail majority vote rule hear soon possible preferably end weekend vote go way complaining allowed like tend great weekend great golf great fishing great shopping whatever make happy bobby"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# get the most accurate model:\n",
    "best_model = modelEval.get_most_accurate_model()\n",
    "\n",
    "#test the model\n",
    "df_spam = vectors.df[vectors.df['label'] == 'spam']\n",
    "df_ham = vectors.df[vectors.df['label'] == 'ham']\n",
    "\n",
    "num_of_test = 3\n",
    "\n",
    "# Test spam prediction\n",
    "for i in range(0, num_of_test):\n",
    "    modelEval.test(best_model, df_spam['text_clean'].iloc[i], df_spam['label'].iloc[i])\n",
    "    print(\"=\"*100)\n",
    "\n",
    "# Test ham prediction\n",
    "for i in range(0, num_of_test):\n",
    "    modelEval.test(best_model, df_ham['text_clean'].iloc[i], df_ham['label'].iloc[i])\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866054b4",
   "metadata": {},
   "source": [
    "### 12. Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53854de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_frequent_words (__main__.TestSuite) ... ERROR\n",
      "test_lemmatization (__main__.TestSuite) ... ERROR\n",
      "test_stopwords (__main__.TestSuite) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_frequent_words (__main__.TestSuite)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-25-a686c214064c>\", line 27, in test_frequent_words\n",
      "    DataCleaner(df_frequentwords_test).clean_most_frequent_words()\n",
      "  File \"<ipython-input-3-5777b4876ec8>\", line 4, in __init__\n",
      "    self.df = analyzed_data.df\n",
      "  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 5465, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'df'\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_lemmatization (__main__.TestSuite)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-25-a686c214064c>\", line 9, in test_lemmatization\n",
      "    DataCleaner(df_lem_test).lemmatization()\n",
      "  File \"<ipython-input-3-5777b4876ec8>\", line 4, in __init__\n",
      "    self.df = analyzed_data.df\n",
      "  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 5465, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'df'\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_stopwords (__main__.TestSuite)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-25-a686c214064c>\", line 18, in test_stopwords\n",
      "    DataCleaner(df_stopwords_test).cleaning_stopwords()\n",
      "  File \"<ipython-input-3-5777b4876ec8>\", line 4, in __init__\n",
      "    self.df = analyzed_data.df\n",
      "  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 5465, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'df'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.007s\n",
      "\n",
      "FAILED (errors=3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x2e4026f49d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestSuite(unittest.TestCase):\n",
    "    \n",
    "    # testing the methods that normalize the text before its analysis\n",
    "    def test_lemmatization(self):   \n",
    "        \n",
    "        lem_test_data = {'text_clean':['multiple scenarios', 'advanced technologies']}\n",
    "        df_lem_test = pd.DataFrame(lem_test_data)\n",
    "        \n",
    "        DataCleaner(df_lem_test).lemmatization()\n",
    "        self.assertEqual('multiple scenario', df_lem_test.iloc[0,0])\n",
    "        self.assertEqual('advanced technology', df_lem_test.iloc[1,0])\n",
    "        \n",
    "    def test_stopwords(self):\n",
    "        \n",
    "        stopwords_test_data = {'text_clean': ['me myself and i', 'it is all the two of us have']}\n",
    "        df_stopwords_test = pd.DataFrame(stopwords_test_data)\n",
    "        \n",
    "        DataCleaner(df_stopwords_test).cleaning_stopwords()\n",
    "        self.assertEqual('', df_stopwords_test.iloc[0,0])\n",
    "        self.assertEqual('two us', df_stopwords_test.iloc[1,0])\n",
    "        \n",
    "    def test_frequent_words(self):\n",
    "        \n",
    "        frequentwords_test_data = {'text_clean': ['test ' * 1001, 'test1 test1']}\n",
    "        df_frequentwords_test = pd.DataFrame(frequentwords_test_data)\n",
    "        \n",
    "        DataCleaner(df_frequentwords_test).clean_most_frequent_words()\n",
    "        self.assertEqual('', df_frequentwords_test.iloc[0,0])\n",
    "        self.assertEqual('test1 test1', df_frequentwords_test.iloc[1,0])\n",
    "\n",
    "# running all the defined tests\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e5f44b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
