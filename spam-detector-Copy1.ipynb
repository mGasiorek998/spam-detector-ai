{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b9f7ed",
   "metadata": {},
   "source": [
    "# Uczenie maszynowe w Python - Zaliczenie\n",
    "## E-mail spam classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf91c9",
   "metadata": {},
   "source": [
    "## Important \n",
    "<ul>\n",
    "    <li>DodaÄ‡ klasy: moje propozycje to:</li>\n",
    "    <ul>\n",
    "        <li>Data Analizer - dostaje dane posiada funckje analize() i zwrca wszelkie informacje o danych (shape, ham amout, spam amount, words per email, words in total)</li>\n",
    "        <li>DataCleaner - dostaje columne texts i posiada funkcje clean(), zwraca text_clean</li>\n",
    "        <li>ModelTester</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "328badd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\piotr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\piotr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from textblob import Word\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeddca4",
   "metadata": {},
   "source": [
    "## 1. Analysis of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e781aca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "     id label                                               text  label_num\n",
      "0   605   ham  Subject: enron methanol ; meter # : 988291\\nth...          0\n",
      "1  2349   ham  Subject: hpl nom for january 9 , 2001\\n( see a...          0\n",
      "2  3624   ham  Subject: neon retreat\\nho ho ho , we ' re arou...          0\n",
      "3  4685  spam  Subject: photoshop , windows , office . cheap ...          1\n",
      "4  2030   ham  Subject: re : indian springs\\nthis deal is to ...          0\n",
      "********************************************************************************\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "The dataset contains 5171 emails, 3672 of which are labeled as non-spam and 1499 as spam. Between the emails, theres a total of 1083244 words, with an average of ~209.48 words per email.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class DataPreparator:\n",
    "    \n",
    "    def __init__(self, file_name):\n",
    "        self.df = pd.read_csv(file_name)\n",
    "        self.email_amount = 0\n",
    "        self.ham_amount = 0\n",
    "        self.spam_amount = 0\n",
    "        self.word_count = 0\n",
    "        self.avg_words = 0       \n",
    "        \n",
    "    # print out the information about the dataset\n",
    "    def __str__(self):\n",
    "        mark_line = \"-\" * 120\n",
    "        data_info = f'The dataset contains {self.email_amount} emails, {self.ham_amount} ' \\\n",
    "                    f'of which are labeled as non-spam and {self.spam_amount} as spam. Between the emails, ' \\\n",
    "                    f'theres a total of {self.word_count} words,' \\\n",
    "                    f' with an average of ~{self.avg_words} words per email.'\n",
    "        \n",
    "        info = \"\\n\".join([mark_line, data_info, mark_line])\n",
    "        return info\n",
    "            \n",
    "    def analize(self):\n",
    "        \n",
    "        # label the unnamed column with ids\n",
    "        self.df.rename(columns={'Unnamed: 0': 'id'}, inplace = True)\n",
    "        print(80 * \"*\")\n",
    "        print(self.df.head())\n",
    "        print(80 * \"*\")\n",
    "\n",
    "        # amount of emails in the dataset\n",
    "        self.email_amount = self.df.shape[0]\n",
    "        \n",
    "        \n",
    "        # Series with the amount of words in each email and their total count\n",
    "        words_in_emails = (self.df['text'].apply(lambda x: len(str(x).split(\" \"))))\n",
    "        self.word_count = np.sum(words_in_emails)\n",
    "        \n",
    "        # average amount of words in each email\n",
    "        self.avg_words = round(np.divide(self.word_count, self.email_amount), 2)\n",
    "        \n",
    "        # amount of spam and ham emails \n",
    "        ham_and_spam_amount= self.df.groupby(['label']).count()\n",
    "        self.ham_amount = ham_and_spam_amount.iloc[0, 0]\n",
    "        self.spam_amount = ham_and_spam_amount.iloc[1, 0]\n",
    "\n",
    "\n",
    "\n",
    "first_step = DataPreparator(\"spam_ham_dataset.csv\")\n",
    "\n",
    "first_step.analize()\n",
    "\n",
    "\n",
    "print(first_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "71a98242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "     id label                                               text  label_num  \\\n",
      "0   605   ham  Subject: enron methanol ; meter # : 988291\\nth...          0   \n",
      "1  2349   ham  Subject: hpl nom for january 9 , 2001\\n( see a...          0   \n",
      "2  3624   ham  Subject: neon retreat\\nho ho ho , we ' re arou...          0   \n",
      "3  4685  spam  Subject: photoshop , windows , office . cheap ...          1   \n",
      "4  2030   ham  Subject: re : indian springs\\nthis deal is to ...          0   \n",
      "\n",
      "                                          text_clean  \n",
      "0  subject enron methanol meter this is a follow ...  \n",
      "1  subject hpl nom for january see attached file ...  \n",
      "2  subject neon retreat ho ho ho we re around to ...  \n",
      "3  subject photoshop windows office cheap main tr...  \n",
      "4  subject re indian springs this deal is to book...  \n",
      "********************************************************************************\n",
      "     id label                                               text  label_num  \\\n",
      "0   605   ham  Subject: enron methanol ; meter # : 988291\\nth...          0   \n",
      "1  2349   ham  Subject: hpl nom for january 9 , 2001\\n( see a...          0   \n",
      "2  3624   ham  Subject: neon retreat\\nho ho ho , we ' re arou...          0   \n",
      "3  4685  spam  Subject: photoshop , windows , office . cheap ...          1   \n",
      "4  2030   ham  Subject: re : indian springs\\nthis deal is to ...          0   \n",
      "\n",
      "                                          text_clean  \n",
      "0  subject enron methanol meter follow note gave ...  \n",
      "1  subject hpl nom january see attached file hpln...  \n",
      "2  subject neon retreat ho ho ho around wonderful...  \n",
      "3  subject photoshop windows office cheap main tr...  \n",
      "4  subject indian springs deal book teco pvr reve...  \n",
      "********************************************************************************\n",
      "0    subject enron methanol meter follow note gave ...\n",
      "1    subject hpl nom january see attached file hpln...\n",
      "2    subject neon retreat ho ho ho around wonderful...\n",
      "3    subject photoshop window office cheap main tre...\n",
      "4    subject indian spring deal book teco pvr reven...\n",
      "5    subject ehronline web address change message i...\n",
      "6    subject spring saving certificate take save us...\n",
      "7    subject looking medication best source difficu...\n",
      "8    subject noms actual flow agree forwarded melis...\n",
      "9    subject nomination oct see attached file hplnl...\n",
      "Name: text_clean, dtype: object\n",
      "********************************************************************************\n",
      "0    methanol follow note gave monday preliminary f...\n",
      "1                       nom january file hplnol hplnol\n",
      "2    neon retreat ho ho ho around wonderful year ne...\n",
      "3    photoshop window office cheap main trending ab...\n",
      "4    indian spring book teco pvr revenue understand...\n",
      "Name: text_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "class DataCleaner:\n",
    "    \n",
    "    def __init__(self, analyzed_data):\n",
    "        self.df = analyzed_data.df\n",
    "        \n",
    "    \n",
    "    def applying_clean_contents(self):\n",
    "        \n",
    "        # apply the function to each row of the dataframe\n",
    "        self.df['text_clean'] = self.df['text'].apply(lambda string: clean_contents(string))\n",
    "        print(80 * \"*\")\n",
    "        print(self.df.head())  # printing 5 rows after\n",
    "        \n",
    "   \n",
    "    def cleaning_stopwords(self):\n",
    "        stop = stopwords.words('english')\n",
    "        self.df['text_clean'] = self.df['text_clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "        print(80 * \"*\")\n",
    "        print(self.df.head())\n",
    "    \n",
    "    # lemmatize the cleaned email contents\n",
    "    def lemmatization(self):\n",
    "        self.df['text_clean'] = self.df['text_clean'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "        # show the first 10 rows after lemmatizing\n",
    "        print(80 * \"*\")\n",
    "        print(self.df['text_clean'].head(10))\n",
    "        \n",
    "    def clean_most_frequent_words(self):\n",
    "        \n",
    "        # define the most frequent words in the emails\n",
    "        frequent_words = pd.Series(' '.join(self.df['text_clean']).split()).value_counts()\n",
    "        frequent_words = frequent_words[frequent_words > 1000]\n",
    "        \n",
    "        # remove them from the email contents\n",
    "        frequent_words = list(frequent_words.index)\n",
    "        self.df['text_clean'] = self.df['text_clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in frequent_words))\n",
    "        print(80 * \"*\")\n",
    "        print(self.df['text_clean'].head())\n",
    "        \n",
    "# clean the contents of each email\n",
    "def clean_contents(string, reg = RegexpTokenizer(r'[a-z]+')):\n",
    "    string = string.lower()\n",
    "    tokens = reg.tokenize(string)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "        \n",
    "second_step = DataCleaner(result)\n",
    "\n",
    "second_step.applying_clean_contents()\n",
    "second_step.cleaning_stopwords()\n",
    "second_step.lemmatization()\n",
    "second_step.clean_most_frequent_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0344de50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Plots:\n",
    "    \n",
    "    def __init__(self, cleaned_data):\n",
    "        self.df = cleaned_data.df\n",
    "        \n",
    "    def show_plot_second_step(self):\n",
    "        self.df['label'].value_counts().plot.bar(color = [\"g\",\"r\"])\n",
    "        plt.title('Total number of ham and spam in the dataset')\n",
    "        plt.show()\n",
    "        \n",
    "plot = Plots(second_step)\n",
    "plot.show_plot_second_step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    \n",
    "    def __init__(self, cleaned_data):\n",
    "        self.df = cleaned_data.df\n",
    "        \n",
    "    def vectorization:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c53ab407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaihmqv', 'aalland', 'aamlrg', 'aare', 'aaron', 'aavilable', 'ab', 'ababa', 'aback', 'abacus', 'abandon', 'abandoned', 'abasement', 'abashed', 'abater', 'abb', 'abbasi', 'abbey', 'abbott', 'abbreviation']\n"
     ]
    }
   ],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(second_step.df.text_clean)\n",
    " \n",
    "# Get the categories\n",
    "y = second_step.df.label\n",
    "# Words exctracted from text_clean\n",
    "print(cv.get_feature_names()[10:50:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e9aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
