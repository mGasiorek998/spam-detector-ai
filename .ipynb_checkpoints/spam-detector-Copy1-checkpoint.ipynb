{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b9f7ed",
   "metadata": {},
   "source": [
    "# Uczenie maszynowe w Python - Zaliczenie\n",
    "## E-mail spam classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf91c9",
   "metadata": {},
   "source": [
    "## Important \n",
    "<ul>\n",
    "    <li>DodaÄ‡ klasy: moje propozycje to:</li>\n",
    "    <ul>\n",
    "        <li>Data Analizer - dostaje dane posiada funckje analize() i zwrca wszelkie informacje o danych (shape, ham amout, spam amount, words per email, words in total)</li>\n",
    "        <li>DataCleaner - dostaje columne texts i posiada funkcje clean(), zwraca text_clean</li>\n",
    "        <li>ModelTester</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328badd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\piotr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\piotr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from textblob import Word\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeddca4",
   "metadata": {},
   "source": [
    "## 1. Analysis of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e781aca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 label                                               text  \\\n",
      "0         605   ham  Subject: enron methanol ; meter # : 988291\\nth...   \n",
      "1        2349   ham  Subject: hpl nom for january 9 , 2001\\n( see a...   \n",
      "2        3624   ham  Subject: neon retreat\\nho ho ho , we ' re arou...   \n",
      "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
      "4        2030   ham  Subject: re : indian springs\\nthis deal is to ...   \n",
      "\n",
      "   label_num  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          1  \n",
      "4          0  \n",
      "     id label                                               text  label_num\n",
      "0   605   ham  Subject: enron methanol ; meter # : 988291\\nth...          0\n",
      "1  2349   ham  Subject: hpl nom for january 9 , 2001\\n( see a...          0\n",
      "2  3624   ham  Subject: neon retreat\\nho ho ho , we ' re arou...          0\n",
      "3  4685  spam  Subject: photoshop , windows , office . cheap ...          1\n",
      "4  2030   ham  Subject: re : indian springs\\nthis deal is to ...          0\n"
     ]
    }
   ],
   "source": [
    "class DataPreparator:\n",
    "    \n",
    "    def __init__(self, file_name):\n",
    "        self.df = pd.read_csv(file_name)\n",
    "        self.email_amount = 0\n",
    "        self.ham_amount = 0\n",
    "        self.spam_amount = 0\n",
    "        self.word_count = 0\n",
    "        self.avg_words = 0       \n",
    "        \n",
    "    # print out the information about the dataset\n",
    "    def __str__(self):\n",
    "        mark_line = \"-\" * 20\n",
    "        data_info = f'The dataset contains {self.email_amount} emails, {self.ham_amount} ' \\\n",
    "                    f'of which are labeled as non-spam and {self.spam_amount} as spam. Between the emails, ' \\\n",
    "                    f'theres a total of {self.word_count} words,' \\\n",
    "                    f' with an average of ~{self.avg_words} words per email.'\n",
    "        \n",
    "        info = \"\\n\".join([mark_line, data_info, mark_line])\n",
    "        return info\n",
    "        \n",
    "    def print_rows(self):\n",
    "        print(df.head())\n",
    "    \n",
    "    def analize(self):\n",
    "        \n",
    "        # label the unnamed column with ids\n",
    "        self.df.rename(columns={'Unnamed: 0': 'id'}, inplace = True)\n",
    "\n",
    "        # amount of emails in the dataset\n",
    "        self.email_amount = self.df.shape[0]\n",
    "        \n",
    "        \n",
    "        # Series with the amount of words in each email and their total count\n",
    "        words_in_emails = (self.df['text'].apply(lambda x: len(str(x).split(\" \"))))\n",
    "        self.word_count = np.sum(words_in_emails)\n",
    "        \n",
    "        # average amount of words in each email\n",
    "        avg_words = round(np.divide(self.word_count, self.email_amount), 2)\n",
    "        \n",
    "        # amount of spam and ham emails \n",
    "        ham_and_spam_amount= self.df.groupby(['label']).count()\n",
    "        ham_amount = ham_and_spam_amount.iloc[0, 0]\n",
    "        spam_amount = ham_and_spam_amount.iloc[1, 0]\n",
    "  \n",
    "# def clean_contents(string, reg = RegexpTokenizer(r'[a-z]+')):\n",
    "#     string = string.lower()\n",
    "#     tokens = reg.tokenize(string)\n",
    "#     return \" \".join(tokens)\n",
    "    \n",
    "    \n",
    "    def text_cleaning(self):\n",
    "        self.df['text_clean'] = self.df['text'].apply(lambda string: clean_contents(string))\n",
    "\n",
    "\n",
    "result = DataPreparator(\"spam_ham_dataset.csv\")\n",
    "print(result.df.head())\n",
    "result.analize()\n",
    "# result.df.rename(columns={'Unnamed: 0': 'id'}, inplace = True)\n",
    "print(result.df.head())\n",
    "# result.df.shape[0]\n",
    "# result = DataPreparator()\n",
    "# result.analize()\n",
    "# result.print_rows()\n",
    "\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a98242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0344de50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
